문자열 -> 랜덤포레스트 사용

당근: 이런 분이면 더 좋아요!
1 판다스 잘 쓰냐
2 자동화 
3 코드 포트폴리오
4 협업능력

평가: 해석 -> 향상
성능 향상은 반드시 평가를 동반한다
평가없는 향상은 사기꾼이다.

저는 x라는 기능을 구현하여 y만큼 성능을 향상했고 z만큼 팀에 기여했습니다.
향후에는 얼마얼마 정도까지 갈 것으로 예상됩니다.

AI: 이걸 왜 배워야 하는가?

매개변수 설정: 하이퍼 파라미터 튜닝 -> 기계가 결정해주지 않는다.

정성적인 것은 성능 평가가 불가능하다: 비지도
알고리즘: 지도학습에 몰빵

평가 -> 교차검증(K-교차검증): 데이터가 다 완비가 되어있어야 하고 시간 걸림
		(1) 임의성: 데이터를 섞는 과정, 데이터를 분리하는 과정 여러번
		(2) HOW
		
score 가 뭘 뜻하는 거지?
구글: sklearn score
cross_val_score: 사용하는 알고리즘이 score 값을 계산할 수 있는 것
-> 에러: 구글링해서 score() 있는지 체크
-> 다른 형태의 평가 지표를 줄 확률이 높다

모델이 프레이닝 데이터에 민감한 경우: 모델 매개변수 따라서 격차 10% 이상

under fitting: 교차검증

교차검증 
-> 평균값을 알 수 있다.
-> 모델의 민감도를 파악할 수 있다
-> 데이터셋이 아주 작을 때 유용

세부 데이터가 필요할 때: cross_validate (걸리는 시간 등등)

model_selection -> KFold

LOOCV: 적은 데이터를 튀구는 거지

임의 분할 교차 검증
-> 잘 섞어서 나눠서 잘 나오면 일반적으로 잘 나오지 않을까?

[목금 한상곤 교수님 수업 예복습 완료 in 수요일] #####

교차검증을 통해서 평가할 수 있다: 평가(k번 가능)

완전탐색: 시간이 너무 걸린다
-> 이 컨셉을 유지한채 속도를 보장하는 방법은?
-> 스레드 사용
-> 대신 돌려주는 애? : 스레드 몇개? 있는 값 조합해서 넣어주어야

GridSearchCV 안 쓰고 RandomGridSearch 사용

1. 전장 결정: 지도학습? 비지도학습? (GridSearch 는 비지도 못씀)
2. 

GridSearch: 오버피팅 문제
8대2

RandomizeSearchCV: 사용해서 시간 줄이고 성능 소폭 하락
n_jobs: 현재 이걸 몇개의 스레드로 할 것인가 (CPU갯수 -1 )

GridSearchCV(SVC(),...) ->  SVC: 아무것도 집어넣지마라

평가 -> 그리드서치 -> 최종평가 + 매개변수(best_params_)

혼돈 행렬

어떤 모델에 어떤 매개변수를 선택해야 좋은 걸 선택한 것인가
F1 값
분류 모델에서는 네개 값(정확도, 재현율, 정밀도, F1)을 다 봐야한다.

회귀보다 분류 선호
-> 오차값에 의사결정권자들이 지나치게 집중

평가 -> 회귀
	 -> 분류 -> 이진: 혼동 행렬(정밀, 재현, 정확, F1) + ROC: 직접 구하는 것 아님
	         -> 다진: Top-K 사용(가장 분류가 잘된 K개)

적당한 손실을 보면서 최대치를 찾아야 한다

-

6장. 알고리즘 체인과 파이프라인

파이프라인: A 를 집어넣으면 -> B -> C -> D -> 별표
-> 전체 시스템을 반복적으로 개선하기 위해서 배운다

파이프라인 구성을 위해서 반드시 생각할 것: 반환값
너의 프로그램의 결과가 다른 사람의 프로그램의 입력이 되도록 하라
-> 함수는 가능한 작게 만든다
-> scikit 파이프라인: scikit only
-> 파이썬 반환값: 단순 숫자, 문자 제외하면 튜플이다. dictionary (list X)


